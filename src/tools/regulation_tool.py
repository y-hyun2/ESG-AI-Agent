import os
import time
import json
import schedule
import requests
import numpy as np
import fitz  # PyMuPDF
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv

# Selenium (ë¸Œë¼ìš°ì € ì œì–´ìš©)
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
try:
    from webdriver_manager.core.utils import ChromeType
except ImportError:  # Older webdriver_manager ë²„ì „ ëŒ€ì‘
    class ChromeType:
        CHROME = "chrome"
        CHROMIUM = "chromium"

# LangChain & AI
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sklearn.metrics.pairwise import cosine_similarity

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "domestic")
HISTORY_DIR = os.path.join(DATA_DIR, "crawling")
HISTORY_FILE = os.path.join(HISTORY_DIR, "crawl_history.json")
LAST_CRAWL_FILE = os.path.join(HISTORY_DIR, "last_crawl.json")
VECTOR_DB_DIR = os.path.join(BASE_DIR, "vector_db", "esg_all")  # ë²¡í„°DB ì €ì¥ ê²½ë¡œ

# [ë³€ê²½] ëª¨ë‹ˆí„°ë§ íƒ€ê²Ÿ ëª©ë¡
# law.go.krì€ ë³„ë„ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ typeì„ êµ¬ë¶„í•˜ê±°ë‚˜ URLë¡œ ì‹ë³„
MINISTRY_TARGETS = [
    {
        "name": "í™˜ê²½ë¶€(êµ­ê°€ë²•ë ¹ì„¼í„°)",
        "url": "https://www.law.go.kr/nwRvsLsPop.do?cptOfi=1482000",
        "type": "LAW_GO_KR",  # ì „ìš© íƒ€ì… ì§€ì •
        "page_param": None
    },
    {
        "name": "ê³ ìš©ë…¸ë™ë¶€(MOEL)",
        "url": "https://www.moel.go.kr/info/lawinfo/lawmaking/list.do", 
        "type": "GENERIC_BOARD",
        "page_param": "pageIndex"
    },
    {
        "name": "êµ­í† êµí†µë¶€(MOLIT)",
        "url": "http://www.molit.go.kr/USR/LEGAL/m_35/lst.jsp",        
        "type": "GENERIC_BOARD",
        "page_param": "page"
    }
]

# [ë³€ê²½] ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤ ë„ë©”ì¸ ëª©ë¡
TRUSTED_NEWS_DOMAINS = [
    "yna.co.kr",       # ì—°í•©ë‰´ìŠ¤
    "mk.co.kr",        # ë§¤ì¼ê²½ì œ
    "hankyung.com",    # í•œêµ­ê²½ì œ
    "sedaily.com",     # ì„œìš¸ê²½ì œ
    "lawtimes.co.kr",  # ë²•ë¥ ì‹ ë¬¸
    "korea.kr",        # ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘
    "chosun.com",      # ì¡°ì„ ì¼ë³´
    "joongang.co.kr",  # ì¤‘ì•™ì¼ë³´
    "donga.com",       # ë™ì•„ì¼ë³´
    "khan.co.kr",      # ê²½í–¥ì‹ ë¬¸
    "etnews.com",      # ì „ìì‹ ë¬¸
    "mt.co.kr",        # ë¨¸ë‹ˆíˆ¬ë°ì´
    "me.go.kr",        # í™˜ê²½ë¶€
    "motie.go.kr",     # ì‚°ì—…í†µìƒìì›ë¶€
    "fsc.go.kr"        # ê¸ˆìœµìœ„ì›íšŒ
]

class RegulationMonitor:
    """
    [ê·œì œ ëª¨ë‹ˆí„°ë§ ì—”ì§„ - AI Enhanced]
    1. Seleniumìœ¼ë¡œ ë³´ê³ ì„œ ë° ë²•ë ¹ì•ˆ ìë™ ë‹¤ìš´ë¡œë“œ (ê¸ˆìœµìœ„/GMI + í™˜ê²½/êµ­í† /ë…¸ë™ë¶€)
    2. êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°(law.go.kr) ì „ìš© í¬ë¡¤ëŸ¬ íƒ‘ì¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ -> íŒŒì¼ ì €ì¥)
    3. GPT-4oë¥¼ ì´ìš©í•´ ë¬¸ì„œì˜ ì¤‘ìš”ë„ í‰ê°€ ë° ì„ ë³„ (Filtering)
    4. ì„ ë³„ëœ ì¤‘ìš” ë¬¸ì„œë§Œ Vector DBì— ìë™ ì €ì¥ (RAG ì¤€ë¹„)
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RegulationMonitor, cls).__new__(cls)
            cls._instance._initialize()
            cls._instance.start_scheduler() # Start background scheduler
        return cls._instance

    def _initialize(self):
        print("âš™ï¸ [RegulationMonitor] ì´ˆê¸°í™” ì¤‘...")
        
        # Embeddings & VectorDBëŠ” í•„ìš”í•  ë•Œ ë¡œë“œ (Lazy Loading)
        self.embeddings = None
        self.vector_db = None
        
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        self.tavily = TavilySearchResults(
            max_results=5,
            include_domains=TRUSTED_NEWS_DOMAINS
        )
        
        os.makedirs(DOWNLOAD_DIR, exist_ok=True)
        os.makedirs(HISTORY_DIR, exist_ok=True)
        os.makedirs(VECTOR_DB_DIR, exist_ok=True)
        
        self.history = self._load_history()

    def _ensure_vector_db(self):
        """Vector DB ë° Embeddings ì§€ì—° ì´ˆê¸°í™”"""
        if self.vector_db is not None:
            return

        print("ğŸ”Œ [System] Embeddings ëª¨ë¸ ë° Vector DB ì´ˆê¸°í™” ì¤‘... (ë‹¤ì†Œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            self.vector_db = Chroma(
                collection_name="esg_regulations",
                embedding_function=self.embeddings,
                persist_directory=VECTOR_DB_DIR
            )
            print("âœ… [System] Vector DB ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None
            self.vector_db = None

    def _load_history(self) -> Dict:
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}

    def _save_history(self):
        try:
            with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _is_processed(self, url: str) -> bool:
        return url in self.history

    def _mark_as_processed(self, url: str, title: str, files: List[str], summary: str = None, origin_url: str = None):
        self.history[url] = {
            "title": title,
            "processed_at": datetime.now().isoformat(),
            "files": files,
            "summary": summary,
            "origin_url": origin_url
        }
        self._save_history()

    def _extract_text_preview(self, file_path: str, max_pages: int = 3) -> str:
        """íŒŒì¼ ë‚´ìš© í”„ë¦¬ë·° ì¶”ì¶œ (PDF ë° TXT ì§€ì›)"""
        text_preview = ""
        try:
            if file_path.lower().endswith('.pdf'):
                doc = fitz.open(file_path)
                for i, page in enumerate(doc):
                    if i >= max_pages: break
                    text_preview += page.get_text()
                doc.close()
            elif file_path.lower().endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    text_preview = f.read(3000) # ì•ë¶€ë¶„ 3000ì
            else:
                text_preview = "(ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤)"
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({os.path.basename(file_path)}): {e}")
        return text_preview

    def _analyze_and_store(self, file_path: str, title: str, source: str) -> tuple[bool, Optional[str]]:
        self._ensure_vector_db()
        if not self.vector_db:
            return False, None

        filename = os.path.basename(file_path)
        print(f"   ğŸ§  [AI ë¶„ì„] '{filename}' ì¤‘ìš”ë„ í‰ê°€ ì¤‘...")

        content_preview = self._extract_text_preview(file_path)
        if not content_preview:
            return False, None

        prompt = f"""
        ë‹¹ì‹ ì€ ê±´ì„¤ì—… ESG ë° ì‚°ì—… ì•ˆì „, í™˜ê²½ ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì¶œì²˜: '{source}'
        ë¬¸ì„œ ì œëª©: '{title}'
        ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:
        {content_preview[:2000]}

        ì´ ë¬¸ì„œê°€ **ê±´ì„¤ì‚¬ ë° í˜‘ë ¥ì‚¬**ì˜ ESG ê²½ì˜, í™˜ê²½ ê·œì œ ì¤€ìˆ˜, ì‚°ì—… ì•ˆì „(ì¤‘ëŒ€ì¬í•´), í˜¹ì€ ì»´í”Œë¼ì´ì–¸ìŠ¤ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” **ì¤‘ìš”í•œ** ë‚´ìš©ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        [íŒë‹¨ ê¸°ì¤€ - ì¤‘ìš” (High Score 7~10)]
        - ê±´ì„¤ í˜„ì¥ ì•ˆì „, ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ê´€ë ¨ ì‚¬í•­
        - íê¸°ë¬¼ ê´€ë¦¬, íƒ„ì†Œ ë°°ì¶œ, ëŒ€ê¸°/ìˆ˜ì§ˆ ì˜¤ì—¼ ë“± ê±´ì„¤ í™˜ê²½ ê·œì œ
        - í•˜ë„ê¸‰ ê³µì •ê±°ë˜, í˜‘ë ¥ì‚¬ ì§€ì› ë“± ê³µê¸‰ë§ ESG
        - ë²•ë¥ /ì‹œí–‰ë ¹ ê°œì •ì•ˆ, ì…ë²•ì˜ˆê³ , ì²˜ë²Œ ê¸°ì¤€ ê°•í™”
        
        [íŒë‹¨ ê¸°ì¤€ - ì œì™¸/ë‚®ìŒ (Score 1~3)]
        - **ì•¼ìƒìƒë¬¼/ë™ë¬¼ ë³´í˜¸** (ê±´ì„¤ í˜„ì¥ í™˜ê²½ì˜í–¥í‰ê°€ì™€ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
        - ë‹¨ìˆœ í–‰ì‚¬, ì„¸ë¯¸ë‚˜, í¬ëŸ¼ ê°œìµœ ì•Œë¦¼
        - ì¥í•™ê¸ˆ, ì¸ì‚¬ ë°œë ¹, ë‚´ë¶€ í–‰ì • ê·œì •(ì§ì œ ë“±)
        - ê±´ì„¤ì—…ê³¼ ë¬´ê´€í•œ íƒ€ ì‚°ì—…(ê¸ˆìœµ ìƒí’ˆ ë‹¨ìˆœ í™ë³´ ë“±) ê·œì œ

        ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
        {{
            "is_important": true/false,
            "score": (1~10),
            "summary": "1. (ì²« ë²ˆì§¸ í•µì‹¬ ë‚´ìš©)\\n2. (ë‘ ë²ˆì§¸ í•µì‹¬ ë‚´ìš©)\\n3. (ì„¸ ë²ˆì§¸ í•µì‹¬ ë‚´ìš©)",
            "category": "ê±´ì„¤ì•ˆì „/í™˜ê²½ê·œì œ/ê³µê¸‰ë§/ê¸°íƒ€"
        }}
        * ì£¼ì˜: 'summary' í•„ë“œëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , 1, 2, 3 ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ 3ì¤„ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.llm.invoke(prompt)
            response_text = response.content.replace("```json", "").replace("```", "").strip()
            analysis = json.loads(response_text)
            
            is_important = analysis.get("is_important", False)
            score = analysis.get("score", 0)
            
            print(f"      ğŸ‘‰ ê²°ê³¼: ì¤‘ìš”ë„ {score}ì ")

            if is_important and score >= 6:
                print(f"      ğŸ’¾ [Vector DB] ì¤‘ìš” ë¬¸ì„œë¡œ ì‹ë³„ë˜ì–´ DBì— ì €ì¥í•©ë‹ˆë‹¤.")
                
                # Use 'summary' from analysis, fallback to 'reason' if old format (though prompt changed)
                summary_text = analysis.get("summary", analysis.get("reason", "ìš”ì•½ ì—†ìŒ"))
                
                full_text = ""
                # PDF ì²˜ë¦¬
                if file_path.lower().endswith('.pdf'):
                    full_doc = fitz.open(file_path)
                    for page in full_doc:
                        full_text += page.get_text()
                    full_doc.close()
                # TXT ì²˜ë¦¬ (law.go.kr ë“±)
                elif file_path.lower().endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        full_text = f.read()
                
                if full_text:
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                    chunks = text_splitter.create_documents(
                        [full_text], 
                        metadatas=[{
                            "source": source,
                            "title": title,
                            "filename": filename,
                            "category": analysis.get("category", "Uncategorized"),
                            "crawled_at": datetime.now().isoformat()
                        }]
                    )
                    self.vector_db.add_documents(chunks)
                    print(f"      âœ… DB ì €ì¥ ì™„ë£Œ ({len(chunks)} chunks)")
                return True, summary_text
            else:
                print(f"      ğŸ—‘ï¸ [Discard] ì¤‘ìš”ë„ê°€ ë‚®ì•„ DBì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False, None

        except Exception as e:
            print(f"      âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False, None

    def _get_chrome_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless=new") 
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--ignore-certificate-errors")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--remote-debugging-port=9222")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        prefs = {
            "download.default_directory": DOWNLOAD_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
            "plugins.always_open_pdf_externally": True,
            "profile.default_content_settings.popups": 0
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        chrome_type = ChromeType.CHROME
        binary_path = os.getenv("CHROME_BINARY")
        if binary_path and "chromium" in binary_path:
            chrome_type = ChromeType.CHROMIUM
        service = ChromeService(ChromeDriverManager(chrome_type=chrome_type).install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        return driver

    def _fetch_law_go_kr(self, driver, target_info: Dict) -> List[Dict]:
        """
        [ì „ìš©] êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°(law.go.kr) í¬ë¡¤ëŸ¬
        - êµ¬ì¡°: ë¦¬ìŠ¤íŠ¸ -> í´ë¦­ -> ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë·°ì–´ (ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ ê¹Œë‹¤ë¡œì›€)
        - ì „ëµ: ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ .txt íŒŒì¼ë¡œ ì €ì¥
        """
        url = target_info["url"]
        source_name = target_info["name"]
        results = []

        print(f"ğŸ“¡ [{source_name}] ì ‘ì† ì¤‘... ({url})")
        try:
            driver.get(url)
            wait = WebDriverWait(driver, 15)
            # law.go.kr ë¦¬ìŠ¤íŠ¸ í…Œì´ë¸” ëŒ€ê¸° (tbody)
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "tbody")))
            
            # ìƒìœ„ 3ê°œ í•­ëª©
            for i in range(3):
                try:
                    row_index = i + 1
                    # ì œëª© ë§í¬ ì°¾ê¸° (ë³´í†µ 2ë²ˆì§¸ tdì˜ a íƒœê·¸, í˜¹ì€ text align left)
                    # law.go.krì€ êµ¬ì¡°ê°€ ê°€ë³€ì ì´ë¼ tr ë‚´ë¶€ì˜ 'a' íƒœê·¸ ì¤‘ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒì„ ì°¾ìŒ
                    row = wait.until(EC.presence_of_element_located(
                        (By.CSS_SELECTOR, f"tbody tr:nth-child({row_index})")
                    ))
                    links = row.find_elements(By.TAG_NAME, "a")
                    
                    target_link = None
                    title = ""
                    for link in links:
                        text = link.text.strip()
                        if text and len(text) > 5: # ì œëª©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§í¬
                            target_link = link
                            title = text
                            break
                    
                    if not target_link: continue

                    unique_key = f"{source_name}_{title}"
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] {source_name}: {title}")
                        continue

                    print(f"   ğŸ” [New] {source_name} ë¶„ì„: {title}")
                    
                    # ìƒì„¸ í˜ì´ì§€ ì§„ì… (law.go.krì€ í´ë¦­ ì‹œ í˜ì´ì§€ ì´ë™/AJAX ë¡œë”©)
                    driver.execute_script("arguments[0].click();", target_link)
                    time.sleep(3) # ë¡œë”© ëŒ€ê¸°
                    
                    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (ë²•ë ¹ ë³¸ë¬¸ ì˜ì—­)
                    # law.go.kr ë³¸ë¬¸ ID í›„ë³´: contentBody, conScroll, viewArea ë“±
                    content_text = ""
                    try:
                        # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                        body_elem = None
                        for selector in ["#contentBody", ".lawCon", "#conScroll", "body"]:
                            try:
                                body_elem = driver.find_element(By.CSS_SELECTOR, selector)
                                if len(body_elem.text) > 100:
                                    break
                            except: continue
                        
                        if body_elem:
                            content_text = body_elem.text
                    except Exception as e:
                        print(f"      âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                    downloaded_files = []
                    if content_text:
                        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_')]).rstrip()
                        file_name = f"{safe_title}.txt"
                        file_path = os.path.join(DOWNLOAD_DIR, file_name)
                        
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(f"ì œëª©: {title}\nì¶œì²˜: {url}\n\n{content_text}")
                        
                        print(f"      âœ… ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {file_name}")
                        downloaded_files.append(file_path)
                        
                        # AI ë¶„ì„ ë° ì €ì¥
                        _, summary = self._analyze_and_store(file_path, title, source_name)
                        
                    self._mark_as_processed(unique_key, title, downloaded_files, summary, origin_url=url)
                    results.append({"source": source_name, "title": title, "files": downloaded_files, "origin_url": url})
                    
                    # ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê¸° (ë’¤ë¡œê°€ê¸° í˜¹ì€ URL ì¬ì ‘ì†)
                    driver.get(url)
                    wait.until(EC.presence_of_element_located((By.TAG_NAME, "tbody")))
                    
                except Exception as e:
                    print(f"      âš ï¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    driver.get(url)
                    time.sleep(2)

        except Exception as e:
            print(f"âŒ [{source_name}] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
        return results

    def _scrape_generic_board(self, driver, target_info: Dict) -> List[Dict]:
        """[ê³µí†µ] ì¼ë°˜ ê²Œì‹œíŒ í¬ë¡¤ë§"""
        base_url = target_info["url"]
        source_name = target_info["name"]
        page_param = target_info.get("page_param")
        results = []

        max_pages = 3 if page_param else 1
        
        for page in range(1, max_pages + 1):
            if page_param:
                sep = "&" if "?" in base_url else "?"
                target_url = f"{base_url}{sep}{page_param}={page}"
            else:
                target_url = base_url

            print(f"ğŸ“¡ [{source_name}] ì ‘ì† ì¤‘ (Page {page})...")
            try:
                driver.get(target_url)
                wait = WebDriverWait(driver, 15)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                
                for i in range(3):
                    try:
                        row_index = i + 1
                        # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ: në²ˆì§¸ í–‰ì˜ ì œëª© ë§í¬ ì°¾ê¸°
                        # êµ¬ì¡°ê°€ ë‹¤ì–‘í•˜ë¯€ë¡œ, í–‰ ë‚´ë¶€ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ aíƒœê·¸ë¥¼ ì œëª©ìœ¼ë¡œ ì¶”ì •
                        row = wait.until(EC.presence_of_element_located(
                            (By.CSS_SELECTOR, f"table tbody tr:nth-child({row_index})")
                        ))
                        links = row.find_elements(By.TAG_NAME, "a")
                        
                        post_link = None
                        title = ""
                        for link in links:
                            text = link.text.strip()
                            if len(text) > 5: # ì œëª©ì¼ ê°€ëŠ¥ì„±
                                post_link = link
                                title = text
                                break
                        
                        if not post_link: continue
                        
                        unique_key = f"{source_name}_{title}"
                        
                        if self._is_processed(unique_key):
                            print(f"   â­ï¸ [Skip] {source_name}: {title}")
                            continue
                            
                        print(f"   ğŸ” [New] {source_name} ë¶„ì„: {title}")
                        
                        driver.execute_script("arguments[0].click();", post_link)
                        time.sleep(2)
                        
                        downloaded_files = []
                        summary = None
                        potential_links = driver.find_elements(By.TAG_NAME, "a")
                        file_links = []
                        for link in potential_links:
                            href = link.get_attribute("href")
                            text = link.text.strip()
                            if href and ("down" in href.lower() or "file" in href.lower() or "download" in href.lower()) and any(ext in text.lower() for ext in ['.pdf', '.hwp', '.doc']):
                                file_links.append(link)
                        
                        for link in file_links[:1]:
                            f_name = link.text.strip()
                            print(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œë„: {f_name}")
                            before_files = set(os.listdir(DOWNLOAD_DIR))
                            driver.execute_script("arguments[0].click();", link)
                            
                            for _ in range(10):
                                time.sleep(1)
                                new_files = set(os.listdir(DOWNLOAD_DIR)) - before_files
                                if new_files:
                                    new_file = list(new_files)[0]
                                    if not new_file.endswith('.crdownload'):
                                        full_path = os.path.join(DOWNLOAD_DIR, new_file)
                                        downloaded_files.append(full_path)
                                        print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {new_file}")
                                        _, summary = self._analyze_and_store(full_path, title, source_name)
                                        break
                        
                        self._mark_as_processed(unique_key, title, downloaded_files, summary, origin_url=target_url)
                        results.append({"source": source_name, "title": title, "files": downloaded_files, "origin_url": target_url})
                        
                        driver.back()
                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                        time.sleep(1)
                        
                    except Exception as e:
                        print(f"      âš ï¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ìŠ¤í‚µ: {e}")
                        if target_url not in driver.current_url:
                            driver.back()
                            time.sleep(1)

            except Exception as e:
                print(f"âŒ [{source_name}] Page {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
        return results

    def _fetch_gmi_reports_selenium(self) -> List[Dict]:
        target_url = "https://www.gmi.go.kr/np/boardList.do?menuCd=2090&seCd=2"
        results = []
        
        print(f"ğŸ“¡ [GMI] ì ‘ì† ë° ìŠ¤ìº” ì‹œì‘ ({target_url})")
        driver = self._get_chrome_driver()
        
        try:
            driver.get(target_url)
            wait = WebDriverWait(driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
            
            for i in range(3):
                try:
                    row_index = i + 1
                    post_link = wait.until(EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, f"table tbody tr:nth-child({row_index}) a")
                    ))
                    
                    title = post_link.text.strip() or driver.execute_script("return arguments[0].innerText;", post_link).strip()
                    unique_key = f"GMI_{title}"
                    
                    if self._is_processed(unique_key):
                        print(f"   â­ï¸ [Skip] ì´ë¯¸ ìˆ˜ì§‘ëœ ë³´ê³ ì„œ: {title}")
                        continue
                        
                    print(f"   ğŸ” [New] ì‹ ê·œ ë³´ê³ ì„œ ë¶„ì„: {title}")
                    driver.execute_script("arguments[0].click();", post_link)
                    time.sleep(2)
                    
                    downloaded_files = []
                    summary = None
                    file_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='downloadAttach']")
                    if not file_links:
                        file_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='FileDown']")

                    for link in file_links:
                        f_name = link.text.strip() or driver.execute_script("return arguments[0].innerText;", link).strip()
                        if 'pdf' in f_name.lower():
                            print(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì‹œë„: {f_name}")
                            before_files = set(os.listdir(DOWNLOAD_DIR))
                            driver.execute_script("arguments[0].click();", link)
                            for _ in range(15):
                                time.sleep(1)
                                new_files = set(os.listdir(DOWNLOAD_DIR)) - before_files
                                if new_files:
                                    downloaded_file = list(new_files)[0]
                                    if not downloaded_file.endswith('.crdownload'):
                                        full_path = os.path.join(DOWNLOAD_DIR, downloaded_file)
                                        downloaded_files.append(full_path)
                                        print(f"      âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_file}")
                                        _, summary = self._analyze_and_store(full_path, title, "GMI")
                                        break
                    
                    self._mark_as_processed(unique_key, title, downloaded_files, summary, origin_url=target_url)
                    results.append({"source": "GMI", "title": title, "files": downloaded_files, "origin_url": target_url})
                    driver.back()
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr")))
                    time.sleep(1)
                except Exception as e:
                    print(f"      âš ï¸ ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if "boardList.do" not in driver.current_url:
                        driver.back()
                        time.sleep(2)
        except Exception as e:
            print(f"âŒ [GMI] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            driver.quit()
        return results

    def _fetch_fsc_reports_selenium(self) -> List[Dict]:
        base_url = "https://www.fsc.go.kr/no010101"
        results = []
        
        print(f"ğŸ“¡ [FSC] ì ‘ì† ë° ìŠ¤ìº” ì‹œì‘ (1~3 í˜ì´ì§€ í™•ì¸)")
        driver = self._get_chrome_driver()
        
        try:
            for page in range(1, 4):
                target_url = f"{base_url}?curPage={page}"
                print(f"   ğŸ“„ FSC Page {page} ìŠ¤ìº” ì¤‘...")
                
                driver.get(target_url)
                wait = WebDriverWait(driver, 20)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".board-list .subject a")))
                
                list_items = driver.find_elements(By.CSS_SELECTOR, ".board-list .subject a")
                keywords = ["ESG", "ê³µì‹œ", "ì§€ì†ê°€ëŠ¥", "ë…¹ìƒ‰", "ê¸°í›„", "íƒì†Œë…¸ë¯¸"]
                
                target_items = []
                for item in list_items:
                    text = item.text.strip()
                    if any(k in text for k in keywords):
                        href = item.get_attribute("href")
                        target_items.append((text, href))
                
                for title, link in target_items:
                    if self._is_processed(link):
                        print(f"      â­ï¸ [Skip] {title}")
                        continue
                    
                    print(f"      ğŸ” [New] ë¶„ì„: {title}")
                    driver.get(link)
                    time.sleep(2)
                    
                    downloaded_files = []
                    summary = None
                    file_links = driver.find_elements(By.CSS_SELECTOR, ".file-list a")
                    
                    for f_link in file_links:
                        f_name = f_link.text.strip()
                        if any(ext in f_name.lower() for ext in ['.pdf', '.hwp']):
                            print(f"         ğŸ“¥ ë‹¤ìš´ë¡œë“œ í´ë¦­: {f_name}")
                            before_files = set(os.listdir(DOWNLOAD_DIR))
                            f_link.click()
                            for _ in range(15):
                                time.sleep(1)
                                new_files = set(os.listdir(DOWNLOAD_DIR)) - before_files
                                if new_files:
                                    new_file = list(new_files)[0]
                                    if not new_file.endswith('.crdownload'):
                                        full_path = os.path.join(DOWNLOAD_DIR, new_file)
                                        downloaded_files.append(full_path)
                                        if new_file.lower().endswith('.pdf'):
                                            _, summary = self._analyze_and_store(full_path, title, "FSC")
                                        break
                    
                    self._mark_as_processed(link, title, downloaded_files, summary, origin_url=link)
                    results.append({"source": "FSC", "title": title, "files": downloaded_files, "origin_url": link})
                    
                    driver.get(target_url)
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".board-list .subject a")))
                    
        except Exception as e:
            print(f"âŒ [FSC] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            driver.quit()
            
        return results

    def _fetch_legal_updates(self) -> List[Dict]:
        results = []
        driver = self._get_chrome_driver()
        try:
            for target in MINISTRY_TARGETS:
                try:
                    # [ë³€ê²½] ì‚¬ì´íŠ¸ íƒ€ì…ì— ë”°ë¼ ì „ìš© í¬ë¡¤ëŸ¬ ì‚¬ìš©
                    if target.get("type") == "LAW_GO_KR":
                        site_results = self._fetch_law_go_kr(driver, target)
                    else:
                        site_results = self._scrape_generic_board(driver, target)
                    results.extend(site_results)
                except Exception as e:
                    print(f"âŒ {target['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            driver.quit()
        return results

    def _get_last_crawl_time(self) -> float:
        try:
            if os.path.exists(LAST_CRAWL_FILE):
                with open(LAST_CRAWL_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get("timestamp", 0.0)
        except:
            pass
        return 0.0

    def _set_last_crawl_time(self):
        try:
            with open(LAST_CRAWL_FILE, 'w') as f:
                json.dump({"timestamp": time.time(), "date": datetime.now().isoformat()}, f)
        except Exception as e:
            print(f"âš ï¸ ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ ì €ì¥ ì‹¤íŒ¨: {e}")

    def crawl_updates(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” í¬ë¡¤ë§ ì‘ì—… (10ì¼ ì£¼ê¸°)"""
        last_crawl = self._get_last_crawl_time()
        elapsed_days = (time.time() - last_crawl) / (3600 * 24)
        
        if elapsed_days < 10:
            print(f"â³ [Scheduler] í¬ë¡¤ë§ ìŠ¤í‚µ (ë§ˆì§€ë§‰ ì‹¤í–‰: {elapsed_days:.1f}ì¼ ì „)")
            return

        print(f"\nğŸ”„ [Scheduler] ì •ê¸° í¬ë¡¤ë§ ì‹œì‘ (10ì¼ ì£¼ê¸°) - {datetime.now().isoformat()}")
        
        # 1. ë³´ê³ ì„œ ìˆ˜ì§‘
        self._fetch_gmi_reports_selenium()
        self._fetch_fsc_reports_selenium()
        
        # 2. ë²•ë ¹ ì—…ë°ì´íŠ¸ ìˆ˜ì§‘
        self._fetch_legal_updates()
        
        self._set_last_crawl_time()
        print("âœ… [Scheduler] ì •ê¸° í¬ë¡¤ë§ ì™„ë£Œ")

    def generate_report(self, query: str = "ESG ê·œì œ ë™í–¥") -> str:
        """ì €ì¥ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¦‰ì‹œ ë¦¬í¬íŠ¸ ìƒì„± (í¬ë¡¤ë§ ìˆ˜í–‰ X)"""
        print(f"ğŸ“Š [Report] ìµœì‹  ë°ì´í„° ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± ìš”ì²­: {query}")
        
        # 0. íˆìŠ¤í† ë¦¬ ìµœì‹ í™” (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì—…ë°ì´íŠ¸ëœ ë‚´ìš© ë°˜ì˜)
        self.history = self._load_history()

        # 1. ìµœê·¼ 10ì¼ ì´ë‚´ ìˆ˜ì§‘ëœ ë°ì´í„° í•„í„°ë§
        recent_reports = []
        recent_files_count = 0
        cutoff_date = datetime.now().timestamp() - (10 * 24 * 3600)
        
        sorted_history = sorted(self.history.items(), key=lambda x: x[1]['processed_at'], reverse=True)
        
        for url, info in sorted_history:
            processed_at = datetime.fromisoformat(info['processed_at']).timestamp()
            if processed_at >= cutoff_date:
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²°ê³¼ì—ì„œ ì œì™¸
                if not info.get('files'):
                    continue
                
                # [Fix] ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì‚¬ìš©ìê°€ ì‚­ì œí–ˆì„ ìˆ˜ë„ ìˆìŒ)
                valid_files = [f for f in info['files'] if os.path.exists(f)]
                if not valid_files:
                    print(f"   âš ï¸ íŒŒì¼ ì†Œì‹¤ë¨ (Skip): {info['title']}")
                    continue
                
                recent_reports.append({
                    "source": "History", 
                    "title": info['title'], 
                    "files": valid_files,
                    "summary": info.get('summary'),
                    "key": url,
                    "origin_url": info.get('origin_url')
                })
                recent_files_count += len(info['files'])
            if len(recent_reports) >= 10: break # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ

        is_fallback = False
        # [Fallback] ìµœê·¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê³¼ê±° ì´ë ¥ì—ì„œ ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜´
        if not recent_reports:
            print("   âš ï¸ ìµœê·¼ ë°ì´í„° ì—†ìŒ. ì´ë ¥ì—ì„œ ìµœì‹  ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
            for url, info in sorted_history:
                if not info.get('files'): continue
                
                recent_reports.append({
                    "source": "History (Fallback)", 
                    "title": info['title'], 
                    "files": info['files'],
                    "summary": info.get('summary'),
                    "key": url,
                    "origin_url": info.get('origin_url')
                })
                # Fallbackì€ 1ê°œë§Œ í™•ì‹¤í•˜ê²Œ ë³´ì—¬ì¤˜ë„ ë¨ (ìš”ì²­ì‚¬í•­: "ì‹œì ì—ì„œ ê°€ì¥ ìµœì‹ ë¬¸ì„œ")
                if len(recent_reports) >= 1: break
            
            if recent_reports:
                is_fallback = True
                result_str = f"## ğŸŒ ESG ê·œì œ & ë²•ë ¹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ (Archive Data)\n"
                result_str += f"> âš ï¸ ìµœê·¼ 10ì¼ ë‚´ ì‹ ê·œ ë¬¸ì„œëŠ” ì—†ì§€ë§Œ, ê°€ì¥ ìµœê·¼ì— ìˆ˜ì§‘ëœ ì¤‘ìš” ë¬¸ì„œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.\n\n"
            else:
                result_str = f"## ğŸŒ ESG ê·œì œ & ë²•ë ¹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸\n"
        else:
            result_str = f"## ğŸŒ ESG ê·œì œ & ë²•ë ¹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ (Latest Data)\n"
            result_str += f"ğŸ“… íŒë‹¨ ê¸°ì¤€: ìµœê·¼ 10ì¼ ì´ë‚´ ìˆ˜ì§‘ëœ ë°ì´í„°\n\n"

        # 2. ìš”ì•½ ì—†ëŠ” ë¬¸ì„œ ìë™ ìš”ì•½ (ì‚¬ìš©ì ìš”ì²­ ëŒ€ì‘)
        for r in recent_reports:
            if not r.get('summary') and r['files']:
                target_file = r['files'][0]
                print(f"   ğŸ¤– [Auto-Sum] '{r['title']}' ìš”ì•½ ìƒì„± ì‹œë„...")
                try:
                    # _analyze_and_store ë¡œì§ì„ ì¼ë¶€ ì¬ì‚¬ìš©í•˜ì—¬ ìš”ì•½ë§Œ ìƒì„±
                    preview = self._extract_text_preview(target_file, max_pages=5)
                    if preview:
                        prompt = f"""
                        ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 3ì¤„ ìš”ì•½í•´ì£¼ì„¸ìš”.
                        ë¬¸ì„œ ì œëª©: {r['title']}
                        ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:
                        {preview[:3000]}
                        
                        [í˜•ì‹]
                        1. (í•µì‹¬ ë‚´ìš© 1)
                        2. (í•µì‹¬ ë‚´ìš© 2)
                        3. (í•µì‹¬ ë‚´ìš© 3)
                        """
                        res = self.llm.invoke(prompt)
                        summary_text = res.content.strip()
                        r['summary'] = summary_text
                        
                        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                        if r.get('key'):
                            self.history[r['key']]['summary'] = summary_text
                            self._save_history()
                        print(f"      âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    print(f"      âš ï¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")

        if recent_reports:
            result_str += "### ğŸ†• ê´€ë ¨ ë³´ê³ ì„œ ë° ë¬¸ì„œ\n"
            for r in recent_reports:
                files_msg = ""
                # ì›ë³¸ URLì´ ìˆìœ¼ë©´ ìš°ì„  í‘œì‹œ
                if r.get('origin_url'):
                    files_msg = f"[ì›ë¬¸ ë³´ê¸°]({r['origin_url']})"
                elif r['files']:
                    links = []
                    for f in r['files']:
                        fname = os.path.basename(f)
                        url = f"http://localhost:8000/static/domestic/{fname}"
                        links.append(f"[ë‹¤ìš´ë¡œë“œ]({url})")
                    files_msg = ", ".join(links)
                else:
                    files_msg = "íŒŒì¼ ì—†ìŒ"
                
                result_str += f"- {r['title']}\n"
                result_str += f"  - ğŸ”— ë§í¬: {files_msg}\n"
                if r.get('summary'):
                    result_str += f"  - ğŸ“ ìš”ì•½:\n{r['summary']}\n"
                else:
                    result_str += f"  - ğŸ“ ìš”ì•½: (ìš”ì•½ ì—†ìŒ)\n"
        else:
            result_str += "### ğŸ†• ìµœì‹  ë³´ê³ ì„œ ë° ë²•ë ¹ ê°œì •ì•ˆ\n"
            result_str += "- ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            
        result_str += "\n### â„¹ï¸ ì°¸ê³ \n"
        result_str += "- ë³¸ ë¦¬í¬íŠ¸ëŠ” ìë™ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.\n"
        
        return result_str

    def start_scheduler(self):
        import threading
        def run_schedule():
            # ì‹œì‘ ì‹œ í•œ ë²ˆ ì²´í¬
            self.crawl_updates()
            while True:
                time.sleep(3600) # 1ì‹œê°„ë§ˆë‹¤ í™•ì¸
                self.crawl_updates()
        
        t = threading.Thread(target=run_schedule, daemon=True)
        t.start()
        print("â° [System] ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")

    # ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€ (í˜¸í™˜ì„±)
    def monitor_all(self, query: str = "ESG ê·œì œ ë™í–¥") -> str:
        print("\n" + "="*50)
        print(f"ğŸ”„ [ëª¨ë‹ˆí„°ë§ ì‹¤í–‰] {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50)

        # 1. ë³´ê³ ì„œ ìˆ˜ì§‘ (GMI, FSC)
        gmi_reports = self._fetch_gmi_reports_selenium()
        fsc_reports = self._fetch_fsc_reports_selenium()
        
        # 2. ë²•ë ¹ ì—…ë°ì´íŠ¸ ìˆ˜ì§‘
        legal_updates = self._fetch_legal_updates()
        
        reports = gmi_reports + fsc_reports + legal_updates
        
        # 3. ë‰´ìŠ¤ ê²€ìƒ‰
        news_results = []
        if os.getenv("TAVILY_API_KEY"):
            queries = list(set([query, "ESG ê³µì‹œ ì˜ë¬´í™”", "í™˜ê²½ë¶€ ì…ë²•ì˜ˆê³ ", "ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ê°œì •"]))
            for q in queries:
                try:
                    raw = self.tavily.invoke(q)
                    for item in raw:
                        news_results.append({
                            "title": item['content'][:30] + "...", 
                            "content": item['content'],
                            "url": item['url'],
                            "source": "Web News"
                        })
                except Exception as e:
                    print(f"âš ï¸ Tavily ê²€ìƒ‰ ì‹¤íŒ¨ ({q}): {e}")
        
        clean_news = self._deduplicate_news(news_results)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result_str = f"## ğŸŒ ESG ê·œì œ & ë²•ë ¹ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ({time.strftime('%Y-%m-%d')})\n\n"
        
        if reports:
            result_str += "### ğŸ†• ì‹ ê·œ ë³´ê³ ì„œ ë° ë²•ë ¹ ê°œì •ì•ˆ\n"
            for r in reports:
                files_msg = ", ".join([os.path.basename(f) for f in r['files']]) if r['files'] else "íŒŒì¼ ì—†ìŒ"
                result_str += f"- **[{r['source']}]** {r['title']}\n"
                result_str += f"  - ğŸ’¾ ë‹¤ìš´ë¡œë“œ: `{files_msg}`\n"
        else:
            result_str += "### ğŸ†• ì‹ ê·œ ë³´ê³ ì„œ ë° ë²•ë ¹ ê°œì •ì•ˆ\n"
            result_str += "- ìƒˆë¡­ê²Œ ë³€ê²½ëœ ì •ì±…ì´ ì—†ìŠµë‹ˆë‹¤.\n"
            
        result_str += "\n### ğŸ“° ì£¼ìš” ë‰´ìŠ¤ ë° ì…ë²• ë™í–¥ (AI ìš”ì•½)\n"
        if clean_news:
            # ìƒìœ„ 3ê°œ ë‰´ìŠ¤ë§Œ ìš”ì•½
            top_news = clean_news[:3]
            for i, n in enumerate(top_news):
                print(f"   ğŸ¤– [AI ìš”ì•½] ë‰´ìŠ¤ {i+1}/{len(top_news)} ìš”ì•½ ì¤‘...")
                try:
                    prompt = f"""
                    ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ 3ì¤„ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
                    
                    ê¸°ì‚¬ ë‚´ìš©: {n['content']}
                    """
                    summary_res = self.llm.invoke(prompt)
                    summary = summary_res.content.strip()
                    
                    result_str += f"**{i+1}. {n['title']}**\n"
                    result_str += f"{summary}\n"
                    result_str += f"ğŸ”— [ì›ë¬¸ ë³´ê¸°]({n['url']})\n\n"
                except Exception as e:
                    print(f"      âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {e}")
                    result_str += f"- {n['content'][:100]}...\n  ğŸ”— [ê¸°ì‚¬]({n['url']})\n"
        else:
            result_str += "- ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        
        print(result_str)
        return result_str

# LangChain Tool Export
_monitor_instance = RegulationMonitor()

@tool
def fetch_regulation_updates(query: str = "ESG regulatory updates") -> str:
    """
    Monitors ESG updates using Selenium and History Tracking to detect NEW reports only.
    Use GPT to filter important documents and store them in Vector DB.
    """
    return _monitor_instance.generate_report(query)

def run_continuously(interval_days: int = 1):
    print(f"\nâ° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘: {interval_days}ì¼ë§ˆë‹¤ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    _monitor_instance.monitor_all()
    schedule.every(interval_days).days.do(_monitor_instance.monitor_all)
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    # [Mode 1] ë‹¨ìˆœ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    print("ğŸ§ª [Test Mode] 1íšŒ í¬ë¡¤ë§ ë° ë¶„ì„ ì‹¤í–‰...")
    _monitor_instance.monitor_all()

    # [Mode 2] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
    # run_continuously(interval_days=1)
